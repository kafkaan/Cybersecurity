---
description: >-
  Web Reconnaissance is the foundation of a thorough security assessment. This
  process involves systematically and meticulously collecting information about
  a target website or web application
cover: ../../.gitbook/assets/1860x1046-blog-Now I See You.jpg
coverY: 0
---

# Information Gathering

<figure><img src="../../.gitbook/assets/image (89).png" alt=""><figcaption></figcaption></figure>

The primary goals of web reconnaissance include:

* **`Identifying Assets`:** Uncovering all publicly accessible components of the target, such as web pages, subdomains, IP addresses, and technologies used. This step provides a comprehensive overview of the target's online presence.
* **`Discovering Hidden Information`:** Locating sensitive information that might be inadvertently exposed, including backup files, configuration files, or internal documentation. These findings can reveal valuable insights and potential entry points for attacks.
* **`Analysing the Attack Surface`:** Examining the target's attack surface to identify potential vulnerabilities and weaknesses. This involves assessing the technologies used, configurations, and possible entry points for exploitation.
* **`Gathering Intelligence`:** Collecting information that can be leveraged for further exploitation or social engineering attacks. This includes identifying key personnel, email addresses, or patterns of behaviour that could be exploited.

## <mark style="color:red;">Types of Reconnaissance</mark>

### <mark style="color:blue;">Active Reconnaissance</mark><br>

<table data-full-width="true"><thead><tr><th width="240">Technique</th><th>Description</th><th>Example</th><th>Tools</th><th>Risk of Detection</th></tr></thead><tbody><tr><td><code>Port Scanning</code></td><td>Identifying open ports and services running on the target.</td><td>Using Nmap to scan a web server for open ports like 80 (HTTP) and 443 (HTTPS).</td><td>Nmap, Masscan, Unicornscan</td><td>High: Direct interaction with the target can trigger intrusion detection systems (IDS) and firewalls.</td></tr><tr><td><code>Vulnerability Scanning</code></td><td>Probing the target for known vulnerabilities, such as outdated software or misconfigurations.</td><td>Running Nessus against a web application to check for SQL injection flaws or cross-site scripting (XSS) vulnerabilities.</td><td>Nessus, OpenVAS, Nikto</td><td>High: Vulnerability scanners send exploit payloads that security solutions can detect.</td></tr><tr><td><code>Network Mapping</code></td><td>Mapping the target's network topology, including connected devices and their relationships.</td><td>Using traceroute to determine the path packets take to reach the target server, revealing potential network hops and infrastructure.</td><td>Traceroute, Nmap</td><td>Medium to High: Excessive or unusual network traffic can raise suspicion.</td></tr><tr><td><code>Banner Grabbing</code></td><td>Retrieving information from banners displayed by services running on the target.</td><td>Connecting to a web server on port 80 and examining the HTTP banner to identify the web server software and version.</td><td>Netcat, curl</td><td>Low: Banner grabbing typically involves minimal interaction but can still be logged.</td></tr><tr><td><code>OS Fingerprinting</code></td><td>Identifying the operating system running on the target.</td><td>Using Nmap's OS detection capabilities (<code>-O</code>) to determine if the target is running Windows, Linux, or another OS.</td><td>Nmap, Xprobe2</td><td>Low: OS fingerprinting is usually passive, but some advanced techniques can be detected.</td></tr><tr><td><code>Service Enumeration</code></td><td>Determining the specific versions of services running on open ports.</td><td>Using Nmap's service version detection (<code>-sV</code>) to determine if a web server is running Apache 2.4.50 or Nginx 1.18.0.</td><td>Nmap</td><td>Low: Similar to banner grabbing, service enumeration can be logged but is less likely to trigger alerts.</td></tr><tr><td><code>Web Spidering</code></td><td>Crawling the target website to identify web pages, directories, and files.</td><td>Running a web crawler like Burp Suite Spider or OWASP ZAP Spider to map out the structure of a website and discover hidden resources.</td><td>Burp Suite Spider, OWASP ZAP Spider, Scrapy (customisable)</td><td>Low to Medium: Can be detected if the crawler's behaviour is not carefully configured to mimic legitimate traffic.</td></tr></tbody></table>

### <mark style="color:blue;">Passive Reconnaissance</mark>

<table data-full-width="true"><thead><tr><th>Technique</th><th>Description</th><th>Example</th><th>Tools</th><th>Risk of Detection</th></tr></thead><tbody><tr><td><code>Search Engine Queries</code></td><td>Utilising search engines to uncover information about the target, including websites, social media profiles, and news articles.</td><td>Searching Google for "<code>[Target Name] employees</code>" to find employee information or social media profiles.</td><td>Google, DuckDuckGo, Bing, and specialised search engines (e.g., Shodan)</td><td>Very Low: Search engine queries are normal internet activity and unlikely to trigger alerts.</td></tr><tr><td><code>WHOIS Lookups</code></td><td>Querying WHOIS databases to retrieve domain registration details.</td><td>Performing a WHOIS lookup on a target domain to find the registrant's name, contact information, and name servers.</td><td>whois command-line tool, online WHOIS lookup services</td><td>Very Low: WHOIS queries are legitimate and do not raise suspicion.</td></tr><tr><td><code>DNS</code></td><td>Analysing DNS records to identify subdomains, mail servers, and other infrastructure.</td><td>Using <code>dig</code> to enumerate subdomains of a target domain.</td><td>dig, nslookup, host, dnsenum, fierce, dnsrecon</td><td>Very Low: DNS queries are essential for internet browsing and are not typically flagged as suspicious.</td></tr><tr><td><code>Web Archive Analysis</code></td><td>Examining historical snapshots of the target's website to identify changes, vulnerabilities, or hidden information.</td><td>Using the Wayback Machine to view past versions of a target website to see how it has changed over time.</td><td>Wayback Machine</td><td>Very Low: Accessing archived versions of websites is a normal activity.</td></tr><tr><td><code>Social Media Analysis</code></td><td>Gathering information from social media platforms like LinkedIn, Twitter, or Facebook.</td><td>Searching LinkedIn for employees of a target organisation to learn about their roles, responsibilities, and potential social engineering targets.</td><td>LinkedIn, Twitter, Facebook, specialised OSINT tools</td><td>Very Low: Accessing public social media profiles is not considered intrusive.</td></tr><tr><td><code>Code Repositories</code></td><td>Analysing publicly accessible code repositories like GitHub for exposed credentials or vulnerabilities.</td><td>Searching GitHub for code snippets or repositories related to the target that might contain sensitive information or code vulnerabilities.</td><td>GitHub, GitLab</td><td>Very Low: Code repositories are meant for public access, and searching them is not suspicious.</td></tr></tbody></table>
